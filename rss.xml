<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Kyle M. Douglass</title><link>http://kmdouglass.github.io/</link><description>Biophysics, optics, and programming.</description><atom:link rel="self" type="application/rss+xml" href="http://kmdouglass.github.io/rss.xml"></atom:link><language>en</language><lastBuildDate>Sat, 24 Jan 2015 09:59:52 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Sending arguments to Python decorators</title><link>http://kmdouglass.github.io/posts/sending-arguments-to-python-decorators.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
&lt;a href="http://simeonfranklin.com/blog/2012/jul/1/python-decorators-in-12-steps/"&gt;Python's decorators&lt;/a&gt; are tools for changing the behavior of a function
without completely recoding it. When we apply a decorator to a
function, we say that the function has been decorated. Strictly
speaking, when we decorate a function, we send it to a wrapper that
returns another function. It's as simple as that.
&lt;/p&gt;

&lt;p&gt;
I was having trouble understanding exactly to which function, the
original or the decorated one, the arguments are sent in a Python
decorated function call. I wrote the following script to better
understand this process (I use Python 3.4):
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inFunction&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;outFunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'The input arguments were:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
	    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="s"&gt; : &lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	&lt;span class="c"&gt;# Return the original function&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;inFunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;outFunction&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;code&gt;wrapper(inFunction)&lt;/code&gt; is a function that accepts another function as
an argument. It returns a function that simply prints the keyword
arguments of &lt;i&gt;inFunction()&lt;/i&gt;, and calls &lt;i&gt;inFunction()&lt;/i&gt; like normal.
&lt;/p&gt;

&lt;p&gt;
To decorate the function &lt;i&gt;add(x = 1, y = 2)&lt;/i&gt; so that its arguments are
printed without recoding it, we normally would place &lt;code&gt;@wrapper&lt;/code&gt; before
its definition. However, let's make the decorator in a way that's
closer to how &lt;i&gt;@&lt;/i&gt; works under the hood:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;In [22]: decoratedAdd = wrapper(add)
In [23]: decoratedAdd(x = 1, y = 24)
The input arguments were:
'y' : 24
'x' : 1
Out[23]: 25
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
When we call &lt;i&gt;decoratedAdd(x = 1, y = 24)&lt;/i&gt;, the arguments are printed
to the screen and we still get the same functionality of &lt;i&gt;add()&lt;/i&gt;. What
I wanted to know was this: are the keyword arguments x = 1, y = 24
bound in the namespace of &lt;i&gt;wrapper()&lt;/i&gt; or in the namespace of
&lt;i&gt;outFunction()&lt;/i&gt;? &lt;b&gt;In otherwords, does &lt;i&gt;wrapper()&lt;/i&gt; at any point know
what the arguments are that I send to the decorated function?&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;
The answer, as it turns out, is no in this case. This is because the
&lt;i&gt;wrapper()&lt;/i&gt; function first returns the decorated function, and then
the arguments are passed into the decorated function. If this order of
operations were flipped, &lt;i&gt;wrapper()&lt;/i&gt; should know that I set x to 1 and
y to 24, but really it doesn't know these details at all.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;In [24]: wrapper(add)(x = 1, y = 24)
The input arguments were:
'y' : 24
'x' : 1
Out[24]: 25
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So, when I call &lt;i&gt;wrapper(add)(x = 1, y = 24)&lt;/i&gt;, first &lt;i&gt;wrapper(add)&lt;/i&gt; is
called, which returns &lt;i&gt;outFunction()&lt;/i&gt;, and then these arguments are
passed to &lt;i&gt;outFunction()&lt;/i&gt;.
&lt;/p&gt;

&lt;p&gt;
Now what happens when I call &lt;code&gt;wrapper(add(x = 1, y = 24))&lt;/code&gt;? When I try
this, the arguments are first passed into add, but then &lt;i&gt;outFunction&lt;/i&gt;
is returned without any arguments applied to it.
&lt;/p&gt;

&lt;p&gt;
This example can give us an idea about the working order of operations
in Python. Here, this example reveals that function calls in Python
are left-associative.
&lt;/p&gt;</description><category>python</category><guid>http://kmdouglass.github.io/posts/sending-arguments-to-python-decorators.html</guid><pubDate>Sat, 24 Jan 2015 07:45:38 GMT</pubDate></item><item><title>Overcoming complexity in biology</title><link>http://kmdouglass.github.io/posts/overcoming-complexity-in-biology.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
I've been sitting in on a short lecture series presented by &lt;a href="http://markolab.bmbcb.northwestern.edu/marko/"&gt;Prof. John
Marko&lt;/a&gt; here at the EPFL. The topic is on the biophysics of DNA and
covers what is probably at least a 25 year span of research that has
emerged on its mechanical and biochemical properties.
&lt;/p&gt;

&lt;p&gt;
DNA amazes me in two different ways: relatively simple physical
theories can explain &lt;i&gt;in vitro&lt;/i&gt; experiments, but establishing a
complete understanding of the behavior of DNA and its associated
proteins (collectively known as chromatin) &lt;i&gt;in vivo&lt;/i&gt; seems at this
point almost hopeless.
&lt;/p&gt;

&lt;p&gt;
So why do I think it's so difficult to establish a complete physical
theory of the nucleus?
&lt;/p&gt;

&lt;p&gt;
Certainly a lot of recent research has helped us to understand parts
of what happens inside the nucleus. Take for example recent
experiments that look at &lt;a href="http://en.wikipedia.org/wiki/Transcription_factor#Accessibility_of_DNA-binding_site"&gt;transcription factor&lt;/a&gt; (TF) binding and the
nuclear architecture. TF binding experiments have helped us understand
the mechanism of how a single transcription factor ``searches'' for a
target site on a chromosome. It undergoes diffusion in the crowded
nuclear environment, occasionally binding to the DNA non-specifically
and sliding along it. We now know that this combined diffusion/sliding
mechanism produces an optimum search strategy.
&lt;/p&gt;

&lt;p&gt;
Studies of nuclear architecture attempt to understand how the long DNA
polymer, which is on the order of &lt;i&gt;one meter&lt;/i&gt; long, is packaged into
the nucleus, which is only about &lt;i&gt;five micrometers&lt;/i&gt; in diameter. This
is nearly six orders of magnitude of compaction. Some current theories
treat the DNA as a hierarchically packaged polymer or a fractal
structure. Interestingly, the fractal model can &lt;a href="http://www.ncbi.nlm.nih.gov/pubmed/24380602"&gt;explain why TF's may
diffuse optimally&lt;/a&gt; and when crowding can hinder their search.
&lt;/p&gt;

&lt;p&gt;
Both of these examples represent a generalization of one particular
phenomenon that occurs inside the nucleus. And even given the enormity
of these works, I think this generalization can be dubious because it
may not apply to all cell types and there may be differences between
cultured cells and those found in an actual organism.
&lt;/p&gt;

&lt;p&gt;
The problem in capturing complete physical models of the nucleus seems
to lie with the philosophy of physics itself: find the most essential
parts of the system and include those in your model, discarding all
irrelevant details. Unfortunately, &lt;i&gt;in vitro&lt;/i&gt; experiments suggest that
every detail seems to matter inside the nucleus. Local salt
concentrations effect electrostatic interactions and entropic binding
between proteins and DNA, the global nuclear architecture has an
effect on single TF's diffusing inside the nucleus, there are a huge
number of proteins that associate with DNA and control the
conformation in &lt;a href="http://www.ncbi.nlm.nih.gov/pubmed/22495300"&gt;toplogically associating domains&lt;/a&gt;. The list goes on and
on.
&lt;/p&gt;

&lt;p&gt;
A common theme to this list that I have described above is that
phenomena at one length scale tend to have a direct impact on those at
another, like the global nuclear architecture affecting a single TF
trajectory. These are hallmarks of complexity: a dependence on the
details of a system and multiscale behavior.
&lt;/p&gt;

&lt;p&gt;
I am currently of the opinion that a complete model of the nucleus,
and probably of other biological systems, must therefore necessarily
abandon one important part of physical theories: reduction to the
simplest possible set of parameters to describe a system. We need to
incorporate all the details across all length scales to reproduce what
exactly is going on.
&lt;/p&gt;

&lt;p&gt;
And if classical physics falls short of this goal, what other
approaches do we then require?
&lt;/p&gt;</description><category>biology</category><category>biophysics</category><category>complexity</category><guid>http://kmdouglass.github.io/posts/overcoming-complexity-in-biology.html</guid><pubDate>Wed, 21 Jan 2015 07:16:04 GMT</pubDate></item><item><title>Learning Python's Multiprocessing Module</title><link>http://kmdouglass.github.io/posts/learning-pythons-multiprocessing-module.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
I've been doing a bit of programming work lately that would greatly
benefit from a speed boost using parallel/concurrent processing
tools. Essentially, I'm doing a &lt;a href="http://www.mathworks.com/help/simulink/examples/parallel-simulations-using-parfor-parameter-sweep-in-normal-mode.html"&gt;parameter sweep&lt;/a&gt; where the values for
two different simulation parameters are input into the simulation and
allowed to run with the results being recorded to disk at the end. The
point is to find out how the simulation results vary with the
parameter values.
&lt;/p&gt;

&lt;p&gt;
In my current code, a new simulation is initialized with each pair of
parameter values inside one iteration of a for loop; each iteration of
the loop is independent of the others. Spreading these iterations over
the 12 cores on my workstation should result in about a 12x decrease
in the amount of time the simulation takes to run.
&lt;/p&gt;

&lt;p&gt;
I've had good success using the &lt;code&gt;parfor&lt;/code&gt; loop construct in Matlab in
the past, but my simulation was written in Python and I want to learn
more about Python's multiprocessing tools, so this post will explore
that module in the context of performing parameter sweeps.
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;Profile the code first to identify bottlenecks&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
First, I profiled my code to identify where any slowdowns might be
occurring in the serial program. I used a great tutorial at the &lt;a href="https://zapier.com/engineering/profiling-python-boss/"&gt;Zapier
Engineering&lt;/a&gt; blog to write a decorator for profiling the main instance
method of my class that was doing most of the work. Surprisingly, I
found that a few numpy methods were taking the most time, namely
&lt;b&gt;norm()&lt;/b&gt; and &lt;b&gt;cross()&lt;/b&gt;. To address this, I directly imported the
Fortran BLAS &lt;b&gt;nrm2()&lt;/b&gt; function using scipy's &lt;b&gt;get_blas_funcs()&lt;/b&gt;
function and hard-coded the cross product in pure Python inside the
method; these two steps alone resulted in a 4x decrease in simulation
time. I suspect the reason for this was because the overhead of
calling functions on small arrays outweighs the increase in speed
using Numpy's optimized C code. I was normalizing single vectors and
taking cross products between two vectors at a time many times during
each loop iteration.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;A brief glance at Python's multiprocessing module&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
&lt;a href="http://pymotw.com/2/multiprocessing/basics.html"&gt;PyMOTW&lt;/a&gt; has a good, minimal description of the main aspects of the
multiprocessing module. They state that the simplest way to create
tasks on different cores of a machine is to create new &lt;b&gt;Process&lt;/b&gt;
objects with target functions. Each object is then set to execute by
calling its &lt;b&gt;start()&lt;/b&gt; method.
&lt;/p&gt;

&lt;p&gt;
The basic example from their site looks like this:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;worker&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""worker function"""&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;'Worker'&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;jobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Process&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;worker&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;jobs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
In this example, it's important to create the Process instances inside
the
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
section of the script because child processes import the script where
the target function is contained. Placing the object instantiation in
this section prevents an infinite, recursive string of such
instantiations. A workaround to this is to define the function in a
different script and import it into the namespace.
&lt;/p&gt;

&lt;p&gt;
To send arguments to the function (&lt;b&gt;worker()&lt;/b&gt; in the example above),
we can use the &lt;b&gt;args&lt;/b&gt; keyword in the Process object instantiation like
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;p = multiprocessing.Process(target=worker, args=(i,))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
A very important thing to note is that the arguments must be objects
that can be pickled using Python's pickle module. If an argument is a
class instance, this means that every attritube of that class must be
pickleable.
&lt;/p&gt;

&lt;p&gt;
An important class in the multiprocessing module is a &lt;b&gt;Pool&lt;/b&gt;. A &lt;a href="https://docs.python.org/3.4/library/multiprocessing.html#multiprocessing.pool.Pool"&gt;Pool&lt;/a&gt;
object controls a pool of worker processes. Jobs can be submitted to
the Pool, which then sends the jobs to the individual workers.
&lt;/p&gt;

&lt;p&gt;
The &lt;b&gt;Pool.map()&lt;/b&gt; achieves the same functionality as Matlab's &lt;b&gt;parfor&lt;/b&gt;
construct. This method essentially applies a function to each element
in an iterable and returns the results. For example, if I wanted to
square each number in a list of integers between 0 and 9 and perform
the square operation on multiple processors, I would write a function
for squaring an argument, and supply this function and the list of
integers to &lt;b&gt;Pool.map()&lt;/b&gt;. The code looks like this:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;funSquare&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;funSquare&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;Design the solution to the problem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
In my parameter sweep, I have two classes: one is an object that I'm
simulating and the other acts as a controller that sends parameters to
the structure and collects the results of the simulation. Everything
was written in a serial fashion and I want to change it so the bulk of
the work is performed in parallel.
&lt;/p&gt;

&lt;p&gt;
After the bottlenecks were identified in the serial code, I began
thinking about how the the problem of parameter sweeps could be
addressed using the multiprocessing module.
&lt;/p&gt;

&lt;p&gt;
The solution requirements I identified for my parameter sweep are as
follows:
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;Accept two values (one for each parameter) from the range of values
to test as inputs to the simulation.
&lt;/li&gt;
&lt;li&gt;For each pair of values, run the simulation as an independent
process.
&lt;/li&gt;
&lt;li&gt;Return the results of the simulation as as a list or Numpy array.
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;
I often choose to return the results as Numpy arrays since I can
easily pickle them when saving to a disk. This may change depending on
your specific problem.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;Implementation of the solution&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
I'll now give a simplified example of how this solution to the
parameter sweep can be implemented using Python's multiprocessing
module. I won't use objects like in my real code, but will first
demonstrate an example where &lt;b&gt;Pool.map()&lt;/b&gt; is applied to a list of
numbers.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""This is the main processing function. It will contain whatever&lt;/span&gt;
&lt;span class="sd"&gt;    code should be run on multiple processors.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;
    &lt;span class="c"&gt;# Example computation&lt;/span&gt;
    &lt;span class="n"&gt;processedData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;processedData&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;processedData&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Define the parameters to test&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;202&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Zip the parameters because pool.map() takes only one iterable&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is a rather silly example of a simulation, but I think it
illustrates the point nicely. In the &lt;b&gt;&lt;span class="underline"&gt;&lt;span class="underline"&gt;main&lt;/span&gt;&lt;/span&gt;&lt;/b&gt; portion of the code, I
first define two lists for each parameter value that I want to
'simulate.' These parameters are zipped together in this example
because &lt;b&gt;Pool.map()&lt;/b&gt; takes only one iterable as its argument. The pool
is opened using with &lt;b&gt;multiprocessing.Pool()&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
Most of the work is performed in the function
&lt;b&gt;runSimulation(params)&lt;/b&gt;. It takes a tuple of two parameters which are
unpacked. Then, these parameters are used in the for loop to build a
list of simulated values which is eventually returned.
&lt;/p&gt;

&lt;p&gt;
Returning to the &lt;b&gt;&lt;span class="underline"&gt;&lt;span class="underline"&gt;main&lt;/span&gt;&lt;/span&gt;&lt;/b&gt; section, each simulation is run on a
different core of my machine using the &lt;b&gt;Pool.map()&lt;/b&gt; function. This
applies the function called &lt;b&gt;runSimulation()&lt;/b&gt; to the values in the
&lt;b&gt;params&lt;/b&gt; iterable. In other words, it calls the code described in
&lt;b&gt;runSimulation()&lt;/b&gt; with a different pair of values in params.
&lt;/p&gt;

&lt;p&gt;
All the results are eventually returned in a list in the same order as
the parameter iterable. This means that the first element in the
&lt;b&gt;results&lt;/b&gt; list corresponds to parameters of 0 and 2 in this example.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-5" class="outline-2"&gt;
&lt;h2 id="sec-5"&gt;Iterables over arbitrary objects&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
In my real simulation code, I use a class to encapsulate a number of
structural parameters and methods for simulating a polymer model. So
long as instances of this class can be &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickled&lt;/a&gt;, I can use them as the
iterable in &lt;b&gt;Pool.map()&lt;/b&gt;, not just lists of floating point numbers.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;simObject&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;objInstance&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""This is the main processing function. It will contain whatever&lt;/span&gt;
&lt;span class="sd"&gt;    code should be run on multiple processors.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;objInstance&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;objInstance&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param2&lt;/span&gt;
    &lt;span class="c"&gt;# Example computation&lt;/span&gt;
    &lt;span class="n"&gt;processedData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;processedData&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;processedData&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Define the parameters to test&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;202&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;objList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="c"&gt;# Create a list of objects to feed into pool.map()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;objList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;simObject&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;p1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;objList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Again, this is a silly example, but it demonstrates that lists of
objects can be used in the parameter sweep, allowing for easy
parallelization of object-oriented code.
&lt;/p&gt;

&lt;p&gt;
Instead of &lt;b&gt;runSimulation()&lt;/b&gt;, you may want to apply an instance method
to a list in &lt;b&gt;pool.map()&lt;/b&gt;. A naïve way to do this is to replace
&lt;b&gt;runSimulation&lt;/b&gt; with with the method name but this too causes
problems. I won't go into the details here, but one solution is to use
an instance's &lt;b&gt;__call__()&lt;/b&gt; method and pass the object instance into
the pool. More details can be found &lt;a href="http://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-pythons-multiprocessing-pool-ma"&gt;here&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-6" class="outline-2"&gt;
&lt;h2 id="sec-6"&gt;Comparing computation times&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-6"&gt;
&lt;p&gt;
The following code makes a rough comparison between computation time
for the parallel and serial versions of &lt;b&gt;map()&lt;/b&gt;:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""This is the main processing function. It will contain whatever&lt;/span&gt;
&lt;span class="sd"&gt;    code should be run on multiple processors.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;
    &lt;span class="c"&gt;# Example computation&lt;/span&gt;
    &lt;span class="n"&gt;processedData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;processedData&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ctr&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;processedData&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'__main__'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Define the parameters to test&lt;/span&gt;
    &lt;span class="n"&gt;param1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;param2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;202&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;multiprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c"&gt;# Parallel map&lt;/span&gt;
    &lt;span class="n"&gt;tic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;toc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c"&gt;# Serial map&lt;/span&gt;
    &lt;span class="n"&gt;tic2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;runSimulation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;toc2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Parallel processing time: &lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;Serial processing time: &lt;/span&gt;&lt;span class="si"&gt;%r&lt;/span&gt;&lt;span class="s"&gt;'&lt;/span&gt;
	  &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;toc&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;tic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;toc2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;tic2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
On my machine, &lt;b&gt;pool.map()&lt;/b&gt; ran in 9.6 seconds, but the serial version
took 163.3 seconds. My laptop has 8 cores, so I would have expected
the speedup to be a factor of 8, not a factor of 16. I'm not sure why
it's 16, but I suspect part of the reason is that measuring system
time using the &lt;b&gt;time.time()&lt;/b&gt; function is not wholly accurate.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-7" class="outline-2"&gt;
&lt;h2 id="sec-7"&gt;Important points&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-7"&gt;
&lt;p&gt;
I can verify that all the cores are being utilized on my machine while
the code is running by using the &lt;a href="http://hisham.hm/htop/"&gt;htop&lt;/a&gt; console program. In some cases,
Python modules like Numpy, scipy, etc. may limit processes in Python
to running on only one core on Linux machines, which defeats the
purpose of writing concurrent code in this case. (See for example &lt;a href="http://stackoverflow.com/questions/15639779/what-determines-whether-different-python-processes-are-assigned-to-the-same-or-d/15641148#15641148"&gt;this
discussion&lt;/a&gt;.) To fix this, we can import Python's &lt;b&gt;os&lt;/b&gt; module to reset
the task affinity in our code:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;system&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"taskset -p 0xff &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getpid&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-8" class="outline-2"&gt;
&lt;h2 id="sec-8"&gt;Conclusions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-8"&gt;
&lt;p&gt;
I think that Matlab's &lt;b&gt;parfor&lt;/b&gt; construct is easier to use because one
doesn't have to consider the nuances of writing concurrent code. So
long as each loop iteration is independent of the others, you simply
write a &lt;b&gt;parfor&lt;/b&gt; instead of &lt;b&gt;for&lt;/b&gt; and you're set.
&lt;/p&gt;

&lt;p&gt;
In Python, you have to prevent infinite, recursive function calls by
placing your code in the &lt;b&gt;&lt;span class="underline"&gt;&lt;span class="underline"&gt;main&lt;/span&gt;&lt;/span&gt;&lt;/b&gt; section of your script or by
placing the function in a different script and importing it. You also
have to be sure that Numpy and other Python modules that use BLAS
haven't reset the core affinity. What you gain over Matlab's
implementation is the power of using Python as a general programming
language with a lot of tools for scientific computing. This and the
multiprocessing module is free; you have to have an institute license
or pay for Matlab's &lt;a href="http://www.mathworks.com/products/parallel-computing/"&gt;Parallel Computing Toolbox&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>computing</category><category>python</category><guid>http://kmdouglass.github.io/posts/learning-pythons-multiprocessing-module.html</guid><pubDate>Mon, 29 Dec 2014 17:42:23 GMT</pubDate></item><item><title>Measurements as processes</title><link>http://kmdouglass.github.io/posts/measurements.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
I work in microscopy, which is one form of optical sensing. In the
sensing field, we are often concerned with making measurements on
some structure so as to learn what it is. Often, I think the word
measurement refers to the dataset that's produced.
&lt;/p&gt;

&lt;p&gt;
I think it can be more effective to think of a measurement as a
process that transforms the structure into the dataset. Why is this
so? Well, to understand what the original structure was, we have to
look at our data and make an inference. If we understand the steps
in the process that took the original structure and turned it into
data, we can apply the inverse of those steps in reverse order to
get the original structure.
&lt;/p&gt;

&lt;p&gt;
Of course, our dataset may only capture some limited aspects of the
original structure, so we may only be able to make probabilistic
statements about what the original structure was.
&lt;/p&gt;

&lt;p&gt;
Take, for example, super-resolution microscopy experiments (SR). In
SR, some feature of a cell is labeled with a discrete number of
fluorescent molecules, then these molecules are localized to a high
precision. The centroids of all the molecules are then convolved
with a Gaussian function (or something similar) with a width equal
to the localization precision to produce a rendered, super-resolved
image of the structure. The measurement process can be thought of
like this:
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;Attach fluorescent molecules to every macromolecule (or randomly
to a subset of macromolecules) in the structure of interest.
&lt;/li&gt;
&lt;li&gt;For every molecule that emits photons during the time of
acqusition by one camera frame, record its true coordinate
positions and the number of detected photons. This can create
multiple localizations that correspond to the same molecule in
multiple camera frames.
&lt;/li&gt;
&lt;li&gt;Remove molecules from the lis that emitted less than some
threshold number of photons. These correspond to molecules with a
signal-to-noise ratio that is too low to be detected.
&lt;/li&gt;
&lt;li&gt;Randomly bump the molecule positions according to a Gaussian
distribution with width equal to the localization precision in
each direction.
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;
This process results in a list of molecule positions that originally
were located on the structure of interest, but were eventually
displaced randomly and filtered out due to various sources of
noise. To understand what the original structure was, we have to
"undo" each of these steps to the best of our abilities.
&lt;/p&gt;

&lt;p&gt;
I think it's interesting to note that everytime a random change to
the original molecule positions occurred, we lose some information
about the structure.
&lt;/p&gt;</description><category>sensing</category><category>super-resolution</category><guid>http://kmdouglass.github.io/posts/measurements.html</guid><pubDate>Thu, 18 Dec 2014 23:00:00 GMT</pubDate></item><item><title>Parallelized confocal microscopy with multiply scattered light</title><link>http://kmdouglass.github.io/posts/confocal-by-scattering.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
My PhD work dealt with the topic of sensing and imaging using light
that had been transmitted through a random medium. This topic is
often applied to practical problems such as imaging through a
turbulent atmosphere or detecting objects, like a tumor, buried
beneath an opaque substance, like skin and muscle tissue. Though I
don't necessarily work in this field anymore, I still follow its
developments occasionally.
&lt;/p&gt;

&lt;p&gt;
A few articles appeared within the last week in journals like &lt;a href="http://www.nature.com/nphoton/journal/v8/n10/full/nphoton.2014.189.html"&gt;Nature&lt;/a&gt;
Photonics]] about the problem of imaging through walls. This problem
has been studied since the late 1980's in a number of papers,
including the notable &lt;a href="http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.61.834"&gt;Feng, Kane, Lee, and Stone&lt;/a&gt; paper and &lt;a href="http://www.sciencedirect.com/science/article/pii/037596019090615U#"&gt;Freund's
discussion&lt;/a&gt; of using cross-correlating two speckle patterns, one of
which is a reference wave, to do a sort of holographic imaging. The
recent work continues from these original ideas and applies them to
microscopy.
&lt;/p&gt;

&lt;p&gt;
One article appeared on the arXiv and is from the &lt;a href="http://arxiv.org/abs/1410.2079"&gt;Mosk and Lagendijk&lt;/a&gt;
camp. In this article, they exploit the optical memory effect,
whereby a speckle pattern generated by the multiple scattering of a
plane wave by a random slab is simply translated as the angle of
incidence of the plane wave is varied. If a thin fluorescent target
is placed directly on the opposite face of the scattering slab, it
will be excited by the speckle pattern and emit a fluorescence
signal that can be captured by an objective. Changing the angle of
incidence of the plane wave then allows for multiple points to scan
the sample in parallel. Ultimately, a number of images are taken
with the sample illuminated by several transversally shifted speckle
patterns and the resulting 4D data cube (corresponding to the
sample's x-y dimensions and the two tilt angles of the incident
plane wave) is used to render an image with improved resolution.
&lt;/p&gt;

&lt;p&gt;
As stated in the article's title, the resolution improvement is
relative to that of a widefield microscope. They obtain an effective
point spread function of about 140 nm and a field of view of 10
microns by 10 microns.
&lt;/p&gt;

&lt;p&gt;
So how does the technique work? My first thought was that the memory
effect is simply another way of saying that the speckle pattern acts
as a number of confocal-like point sources, which essentially means
this is something of a parallelized confocal microscope. However,
I'm not sure this is correct for two reasons: 1) there is no
detection pin hole, and 2) the angle of incidence of the plane wave
is scanned over a small angular range so that the speckle pattern is
simply translated. If the angle of incidence is so great that the
linear change in phase of the plane wave is greater than roughly the
size of the scattering slab, the speckle pattern is no longer simply
translated but changes completely.
&lt;/p&gt;

&lt;p&gt;
In reality, the ultimate resolution of this technique is the average
speckle grain size, which can't be less than about half the
wavelength. This suggests that the angular spectrum of the speckle
pattern is what determines the resolution improvement.
&lt;/p&gt;

&lt;p&gt;
The speckle pattern in a region bounded by the maximum extent of the
memory effect has a fixed angular spectrum and translating the
speckle pattern only changes the phase of the spectrum. So, scanning
a target with a speckle pattern produces beat patterns containing
high spatial frequency information that can propgate on the low
spatial frequency waves that reach the objective. Translating the
speckle pattern then performs a sort of phase-shifting
interferometry that allows for both intensity and phase retrieval of
the object.
&lt;/p&gt;

&lt;p&gt;
Importantly, if the speckle pattern is scanned outside the range of
the memory effect, the speckle's angular spectrum within the region
changes completely so that the original reference wave is lost. The
fact that the object is fluorescent and not simply scattering the
light shouldn't matter if the fluorescence intensity is linear with
the excitation light intensity. However, if the fluorescence has
saturated at the average intensity of the speckle pattern, then I'm
not exactly sure that this technique will work (though maybe some
sort of nonlinear structured illumination microscopy could be
achieved).
&lt;/p&gt;

&lt;p&gt;
Overall it's a neat demonstration and worth the exercise to
understand it, though I'm doubtful that at this point it would be
useful for applications in the life sciences. This is because the
resolution isn't that much better than a spinning disk confocal
microscope, which has a larger field of view and would arguably be
easier to use by biologists.
&lt;/p&gt;</description><category>microscopy</category><category>optics</category><guid>http://kmdouglass.github.io/posts/confocal-by-scattering.html</guid><pubDate>Wed, 08 Oct 2014 22:00:00 GMT</pubDate></item><item><title>An accidental half-wave plate</title><link>http://kmdouglass.github.io/posts/half-wave plate.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
Recently in the lab we made a seemingly minor change to one of our
microscopes that led to a minor problem that lasted for about a
week. Briefly, we introduced a mirror following a periscope that
raises the plane that the laser beams travel in parallel to the
table. The periscope is necessary to bringthe beams out of the plane
containing the beam-combining optics and into the TIRF module port
on our Zeiss inverted microscope for epi-illumination.
&lt;/p&gt;

&lt;p&gt;
We had introduced the mirror to give us one more degree of freedom
for steering the beam, but to do so we had to move the periscope and
turn one of the mirrors by 90 degrees. Unfortunately, after doing
this we found that the optical power leaving the microscope
objective had dropped by a factor of 10, which was insufficient to
do STORM imaging.
&lt;/p&gt;

&lt;p&gt;
We eventually determined that the Zeiss TIRF module contained a
polarizing beam splitter that combined the laser beams with white
light from another port at 90 degrees to the beam path. In the
newsetup, we placed a broadband halfwave plate before the TIRF
module, rotated it while watching a power meter, and were finally
able to get even more power leaving the objective than with the old
setup.
&lt;/p&gt;

&lt;p&gt;
So what had happened to cause the decrease in power by adding
another mirror, and why did introducing the halfwave plate fix the
problem? As it turns out, you can rotate the polarization by 90
degrees using only a pair of mirrors to redirect the beam into a
plane parallel to its original plane of travel but into a direction
perpendicular to its original one. This was what we were doing
before we introduced the mirror. After the new mirror was put in
place, we rotated a mirror of the periscope, which effectively
rotated the laser beam's polarization back into its original
orientation. And, since there was a polarizing beam splitter inside
the TIRF module, the new beam polarization was transmitted through
the module instead of being reflected towards the objective.
&lt;/p&gt;

&lt;p&gt;
The picture below describes how this can happen. Note that the final
beam trajectory is in the direction pointing &lt;i&gt;into&lt;/i&gt; the screen.
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://kmdouglass.github.io/images/polarization_rotator.png" alt="polarization_rotator.png"&gt;&lt;/p&gt;
&lt;/div&gt;</description><category>optics</category><guid>http://kmdouglass.github.io/posts/half-wave plate.html</guid><pubDate>Tue, 05 Aug 2014 22:00:00 GMT</pubDate></item><item><title>The philosophy of -omics studies</title><link>http://kmdouglass.github.io/posts/omics.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
As a physcist, I'm a relative newcomer to the biological
sciences. As a result, many approaches to doing science within these
fields are new and really interesting to me.
&lt;/p&gt;

&lt;p&gt;
One such idea is the &lt;i&gt;-omics&lt;/i&gt; approach to science. Sonja Prohaska
and Peter Stadler provide an insightful and sometimes amusing
intrepetation of &lt;i&gt;-omics&lt;/i&gt; studies in an article in the book
&lt;i&gt;Bioinformatics for Omics Data.&lt;/i&gt; (The PubMed link to the article is
&lt;a href="http://www.ncbi.nlm.nih.gov/pubmed/21370084"&gt;here&lt;/a&gt;, but you can find a pdf by Googling "The use and abuse of
-omes.") According to them, &lt;i&gt;-omics&lt;/i&gt; refers to the research field of
digesting the pile of data coming from measurements of a particular
&lt;i&gt;-ome&lt;/i&gt;, such as the "genome" or "transcriptome." The goal is to
relate the collection of parts within the &lt;i&gt;-ome&lt;/i&gt; to biological
function, or at least to determine function by comparing &lt;i&gt;-omes&lt;/i&gt; of
two different organisms.
&lt;/p&gt;

&lt;p&gt;
The authors explain that &lt;i&gt;-omics&lt;/i&gt; approaches have three components,
which I quote from their article:
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;A suite of (typically high-throughput) technologies address a
well-defined collection of biological objects, the pertinent
&lt;i&gt;-ome&lt;/i&gt;.
&lt;/li&gt;
&lt;li&gt;Both technologically and conceptually there is a cataloging
effort to enumerate and characterize the individual
objects–hopefully coupled with an initiative to make this
catalog available as a database.
&lt;/li&gt;
&lt;li&gt;Beyond the enumeration of objects, one strives to cover the
quantitative aspects of the &lt;i&gt;-ome&lt;/i&gt; at hand.
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;
While grand in scope, these approaches carry difficulties that to me
appear unique. As stated above, &lt;i&gt;-omic&lt;/i&gt; information is acquired
through high-throughput techniques, which means that they generate
very large amounts of data. Of major concern is actually linking
this data to biological function. In other words, scientists must
answer whether the correlations in the data actually allow us to
predict the behavior of a cell or tissue. As might be expected,
generating a complex data set such as is found in &lt;i&gt;-omics&lt;/i&gt; studies
can be quite impressive at first glance. But this complexity may
hide the fact that no biologically relevant conclusions can be drawn
from it.
&lt;/p&gt;

&lt;p&gt;
The authors specifically enumerate four limitations that could
adversely affect &lt;i&gt;-omics&lt;/i&gt; studies:
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;Technical limitations
&lt;/li&gt;
&lt;li&gt;Limitations in the experimental design (such as heavy reliance on
assumptions)
&lt;/li&gt;
&lt;li&gt;Conceptual limitations
&lt;/li&gt;
&lt;li&gt;Limitations in the analysis
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;
Particularly, the conceptual limitations struck me as
intriguing. The authors offered the example of genomics studies in
which the notion of a "gene" is not currently a well-defined
concept. When the concepts underpinning an entire collection of
measurements is unclear, we should question whether the measured
data has the meaning that we think it does.
&lt;/p&gt;

&lt;p&gt;
Overall, I found that this commentary provided an interesting and
sobering view of one particular approach to studying biology. It's
interesting because I suspect that many important biological
questions in the near future will come from taking an integrated and
systems perspective. This perspective will require high-throughput
techniques that carry the same limitations that &lt;i&gt;-omics&lt;/i&gt; studies
have.
&lt;/p&gt;</description><category>biology</category><guid>http://kmdouglass.github.io/posts/omics.html</guid><pubDate>Mon, 31 Mar 2014 22:00:00 GMT</pubDate></item><item><title>Customized Wikibooks are awesome</title><link>http://kmdouglass.github.io/posts/wikibooks.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
Last night the internet failed in our apartment and is still
down. Because of this, I ran into the office today (a Saturday) to
download a bunch of files to do some much-needed background reading
on some topics.
&lt;/p&gt;

&lt;p&gt;
This lead me to an interesting question, one which I had not yet
thought of: how can I download Wikipedia pages in a format that
makes for easy offline access?
&lt;/p&gt;

&lt;p&gt;
A very quick Google search gave me a surprising answer: make your
own book from Wikipedia pages!
&lt;/p&gt;

&lt;p&gt;
Here's how it works. (I'll skip the actual details and &lt;a href="https://en.wikipedia.org/wiki/Help:Books"&gt;let you read
it straight from Wikipedia's help files&lt;/a&gt;. However, I will explain the
process of making your own book.) After you activate the book
creater, you simply navigate to any Wikipedia page and click the
button on the top of the page that asks if you want to add the
current page to your book. Do this for as many pages as you'd like.
&lt;/p&gt;

&lt;p&gt;
When you're ready to access your book, you can do it online, or
export it to a number of formats for offline viewing, such as PDF or
an Open Document format. I thought that the PDF was beautufilly
rendered. It even included all reference information.
&lt;/p&gt;

&lt;p&gt;
There are other options with this tool too, such as sorting your
articles into chapters or naming your book. I believe that you can
share it with others as well.
&lt;/p&gt;

&lt;p&gt;
Cheers to you, Wikipedia, for creating an awesome and useful
feature!
&lt;/p&gt;</description><category>wikipedia</category><guid>http://kmdouglass.github.io/posts/wikibooks.html</guid><pubDate>Fri, 14 Feb 2014 23:00:00 GMT</pubDate></item><item><title>The art of alignment</title><link>http://kmdouglass.github.io/posts/alignment.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
I've been working on realigning one of our STORM microscopy setups
and moving the other over the past couple weeks. As a result, I've
been doing a lot more alignment lately than I normally do, which has
got me thinking about alignment in general.
&lt;/p&gt;

&lt;p&gt;
I've worked in optics labs for nearly ten years now, but I was never
systematically taught how to do alignments. Eventually, I just
figured it out from asking and watching lots of others do it. This
part of experimental optics seems to be something that's passed down
and shared across generations of PhD students and post-docs, like
some sort of cultural heritage preserved by spoken word.
&lt;/p&gt;

&lt;p&gt;
Unfortunately, this fact means that written tutorials or alternative
resources on alignment are scarce. This in turn leads to frustration
for new students who can't easily find someone to show them how to
align a system. I was lucky in my training because both my
undergraduate and graduate programs were in centers dedicated to the
study of optics and optical engineering. Resources were
plentiful. But for a biologist working with a custom microscope
using multiple laser lines, finding someone who is competent in
optics to help realign their system can be highly unlikely.
&lt;/p&gt;

&lt;p&gt;
So how can people who don't have a background in optics be trained
in alignment? I'm certain that written tutorials are &lt;i&gt;not&lt;/i&gt; the best
tool, since aligning an optics setup is a very hands-on job. But, a
tutorial that makes explicit the principles of alignment might make
a good starting point for others.
&lt;/p&gt;

&lt;p&gt;
What are these principles? Below is my rough, initial list that
applies to aligning laser beams. Aligning incoherent systems is
another beast altogether.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;The angle of the beam propagation is just as important as the
position of the beam in a plane perpendicular to the table. In
other words, just because a beam is going through a stopped-down
iris doesn't mean it's traveling where you think it is.
&lt;/li&gt;
&lt;li&gt;You need to measure the beam position in two planes to determine
its angle of propagation.
&lt;/li&gt;
&lt;li&gt;Use "layers of abstraction" to divide sections of the setup into
independent units.
&lt;/li&gt;
&lt;li&gt;Use paired mirrors to get enough degrees of freedom to establish
these abstraction layers.
&lt;/li&gt;
&lt;li&gt;Design feedback mechanisms into the system. In other words, keep
some irises or alignment marks on the walls in place to aid in
future realignments.
&lt;/li&gt;
&lt;/ol&gt;</description><category>optics</category><guid>http://kmdouglass.github.io/posts/alignment.html</guid><pubDate>Sun, 26 Jan 2014 23:00:00 GMT</pubDate></item><item><title>Hello, world!</title><link>http://kmdouglass.github.io/posts/hello-world.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
Well, I've finally got this site off the ground… more or less.
&lt;/p&gt;

&lt;p&gt;
This website is the continuation of my blog, &lt;a href="http://quadrule.blogspot.com"&gt;My Quad-Ruled Life&lt;/a&gt;. On
that site, I explored a number of topics as a graduate student in
optics, while improving my writing as well. As the site progressed
and I matured academically, I found that I was blogging less often,
but still used the blog as a means of exploring ideas. Additionally,
I desired a place to present notes and code to the public in an
easily accessible and open manner.
&lt;/p&gt;

&lt;p&gt;
So, after discovering &lt;a href="http://getnikola.com/"&gt;Nikola&lt;/a&gt; and &lt;a href="http://pages.github.com/"&gt;GitHub Pages&lt;/a&gt;, I quickly hashed
together a fully homemade site that allowed me to keep writing blog
posts and to present some of the fruits of my research freely on the
internet.
&lt;/p&gt;

&lt;p&gt;
I hope you'll find this site useful as a learning resource and as a
means to get to know me better as a scientist.
&lt;/p&gt;

&lt;p&gt;
-Kyle
&lt;/p&gt;</description><category>blog</category><guid>http://kmdouglass.github.io/posts/hello-world.html</guid><pubDate>Fri, 20 Dec 2013 23:00:00 GMT</pubDate></item></channel></rss>