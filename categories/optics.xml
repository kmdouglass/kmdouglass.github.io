<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Kyle M. Douglass (optics)</title><link>http://kmdouglass.github.io/</link><description></description><atom:link href="http://kmdouglass.github.io/categories/optics.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Mon, 04 May 2015 07:45:11 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Can my laser beam be collimated?</title><link>http://kmdouglass.github.io/posts/can-my-beam-be-collimated.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
The fluorescence microscopy setup in my lab requires quite a bit of
power. The minimum irradiance requirement is greater than 1 kW per
square centimeter, and this must cover an area spanning a few tens of
microns across after focusing through the objective. When I was
designing the setup, the highest priority was placed on finding a
cheap laser with as much power as possible at a wavelength of 647 nm;
I considered all other qualities of the laser of secondary importance.
&lt;/p&gt;

&lt;p&gt;
Just like everything else in science, I have learned a very good
lesson from this experience. The laser I decided to purchase is a &lt;a href="http://www.omicron-laser.de/english/lasers/diode-lasers/brixx-lasers/brixx-diode-lasers.html"&gt;800
mW BrixX laser from Omicron&lt;/a&gt;. The cost is under $10,000, which I
consider to be a pretty good deal for the amount of power it puts
out. There is no fiber-coupled version, but all of our lasers are free
space anyway so I did not consider this to be a big problem. The beam
is astigmatic, which is to be expected from a high power laser diode.
&lt;/p&gt;

&lt;p&gt;
The astigmatism is not necessarily a big problem for me, though. What
is surprising to me is how difficult it is to keep the beam &lt;a href="http://en.wikipedia.org/wiki/Collimated_light"&gt;collimated&lt;/a&gt;
over large distances, that is, to keep it roughly the same size as it
propagates. My application requires a fairly small beam size since the
exit pupil of the objective is only 6 millimeters in diameter. With
&lt;a href="http://en.wikipedia.org/wiki/M_squared"&gt;M-squared&lt;/a&gt; values direct from the laser of 12 and 25, it is quite
difficult to keep the beam collimated for a long enough distance to
steer the beam through all the optics and to keep it small enough to
prevent overfilling the objective's exit pupil and losing power.
&lt;/p&gt;

&lt;p&gt;
What I have learned from this is that free-space diode lasers, and
more generally multimode laser beams, require extra consideration to
ensure that they will stay collimated in long setups.
&lt;/p&gt;

&lt;p&gt;
So, how can I predict the distance over which my beam can stay
collimated to better judge how well it will work for my setup? The
first thing to realize is that no beam can stay collimated
forever. Real laser beams experience diffraction, which causes them to
spread as they propagate. This means that the first thing I should do
is to consider the distances spanned by the beam paths. In a
microscopy setup, this path will probably not be longer than a couple
meters. In mine, it's roughly two meters since I am combining a number
of laser beams together and need the extra space.
&lt;/p&gt;

&lt;p&gt;
Once I identify the length scale over which the beam should stay
collimated, I need to examine the beam parameter that is best
associated with collimation. For a pure Gaussian beam, this parameter
is the &lt;a href="http://en.wikipedia.org/wiki/Rayleigh_length"&gt;Rayleigh range&lt;/a&gt;. The Rayleigh range is the distance between the
beam waist and the point where the cross-sectional area of the beam
has doubled, or, equivalently, to the point where the radius of the
beam has increased over the waist radius by a factor of the
\(\sqrt{2}\). For a pure Gaussian beam with a waist radius of \(w_0\) and
a wavelength of \(\lambda\), the Rayleigh range is given by the equation
&lt;/p&gt;


\begin{equation*}
z_R = \frac{\pi w_0^2}{\lambda}
\end{equation*}

&lt;p&gt;
It is represented by the symbol &lt;i&gt;z&lt;sub&gt;R&lt;/sub&gt;&lt;/i&gt; in the figure below from
Wikipedia. The total distance over which the beam will stay collimated
is represented by &lt;i&gt;b&lt;/i&gt; and is just twice the Rayleigh range.
&lt;/p&gt;

&lt;div align="center"&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://kmdouglass.github.io/GaussianBeamWaist.png" alt="GaussianBeamWaist.png"&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;
The Rayleigh range of a Gaussian laser beam should therefore be larger
than the characteristic distance of my setup that I identified in the
previous step. But what about multimode beams like the one from my
laser diode? This is where the concept of an "embedded Gaussian" comes
into play. (For more information about this idea, see &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.3400&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;this tutorial by
Tony Siegman&lt;/a&gt;.) I can predict the collimated distance by computing the
Rayleigh range of an ideal Gaussian beam, and then divide it by the
M&lt;sup&gt;2&lt;/sup&gt; parameter for the beam.
&lt;/p&gt;

&lt;p&gt;
Let's take an example using numbers from my own laser. I will first
pretend there are no collimating optics in the laser, which is not
true but will serve as a good example as to why diode lasers without
collimating optics are not good for free space setups. From the
laser's spec sheet, I know that the M&lt;sup&gt;2&lt;/sup&gt; value in the ``bad'' direction
is 25 and that its waist size, which is probably half the size of the
diode in one principle direction, is 107 microns. Using the above
equation, I get a value of 56 centimeters, which means that an ideal
Gaussian beam with these specs will stay collimated over about half a
meter.
&lt;/p&gt;

&lt;p&gt;
However, my real multimode beam will have a Rayleigh range that is
smaller than this value by a factor of 25, which is only about 2
centimeters. For this reason, diode laser beams almost always have
collimating optics; the beam from the diode itself is highly
divergent.
&lt;/p&gt;

&lt;p&gt;
Returning to my own setup, if I resize the beam using a telescope to
have a waist radius of 2 millimeters so it almost entirely fits inside
the objective's exit pupil, the Rayleigh range of the embedded
Gaussian beam will be almost &lt;i&gt;20 meters&lt;/i&gt;, which is pretty long. The
real beam, however, will have a Rayleigh range of about 780
millimeters meters due to the high M&lt;sup&gt;2&lt;/sup&gt; value. &lt;b&gt;Practically, this means
I have about one meter of collimated laser beam&lt;/b&gt; with which to work
since I have double the Rayleigh range of collimated distance, but
really I want the beam to travel less distance than this.
&lt;/p&gt;

&lt;p&gt;
Fortunately I can shrink the beam path enough that this should not be
a problem, but it does serve as a very good lesson when looking for
lasers for a microscopy application.
&lt;/p&gt;</description><category>microscopy</category><category>optics</category><guid>http://kmdouglass.github.io/posts/can-my-beam-be-collimated.html</guid><pubDate>Mon, 04 May 2015 07:44:42 GMT</pubDate></item><item><title>Relearning paraxial optics</title><link>http://kmdouglass.github.io/posts/relearning-paraxial-optics.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
As a microscopist who designs and builds custom microscopes, I often
find that first order ray tracing is one of the most useful tools I
have. It is most useful to me when I am starting a design or for
checking that I have not made any serious logical blunders when
working at the bench. I also find it to be a good teaching tool and
excellent for communicating my designs to others.
&lt;/p&gt;

&lt;p&gt;
Since I have recently made a lot of back-of-the-envelope ray trace
designs for a microscope I am building at work, I have begun to
critically think about why ray tracing works so well and how it fits
inside the structure of the physical theories of optics.
&lt;/p&gt;

&lt;p&gt;
Ray tracing is a technique derived from &lt;b&gt;paraxial geometrical optics&lt;/b&gt;
and really is just a consequence of the axioms and assumptions in the
development of the theory. When I think back to when I first learned
about paraxial optics, however, I am reminded of severe assumptions
that place limits on the scope of its validity. So how can a theory
that makes such an enormous simplification by treating the
electromagnetic waves described by Maxwell's equations as lines
obeying the rules of geometry still be so useful?
&lt;/p&gt;

&lt;p&gt;
In this next series of blog posts, I want to explore this question and
take the time to relearn paraxial optics with the benefit of
hindsight. Physicists typically learn paraxial optics as their first
theory of optics because it is relatively easy as compared to
electromagnetism and quantum optics. I am curious what aspects of the
theory I can appreciate now that I am familiar with the more advanced
optical theories.
&lt;/p&gt;

&lt;p&gt;
The picture below might also provide a bit of motivation for why this
interests me: we physicists learn about optics theories in a direction
of increasing complexity during our education. However, this approach
also means that we learn the most general theories last, so it is not
obvious why assumptions and approximations are made in the theories we
learn first. The result, I think, leads to a bit of logical
discordance in our minds that can prevent a clear understanding of the
subject. With this series of posts I hope to remove this cognitive
dissonance and improve my understanding of the field I work in.
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://kmdouglass.github.io/order_of_optics_theories.png" alt="order_of_optics_theories.png"&gt;&lt;/p&gt;
&lt;/div&gt;</description><category>optics</category><guid>http://kmdouglass.github.io/posts/relearning-paraxial-optics.html</guid><pubDate>Sun, 19 Apr 2015 09:05:21 GMT</pubDate></item><item><title>Parallelized confocal microscopy with multiply scattered light</title><link>http://kmdouglass.github.io/posts/confocal-by-scattering.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
My PhD work dealt with the topic of sensing and imaging using light
that had been transmitted through a random medium. This topic is
often applied to practical problems such as imaging through a
turbulent atmosphere or detecting objects, like a tumor, buried
beneath an opaque substance, like skin and muscle tissue. Though I
don't necessarily work in this field anymore, I still follow its
developments occasionally.
&lt;/p&gt;

&lt;p&gt;
A few articles appeared within the last week in journals like &lt;a href="http://www.nature.com/nphoton/journal/v8/n10/full/nphoton.2014.189.html"&gt;Nature&lt;/a&gt;
Photonics]] about the problem of imaging through walls. This problem
has been studied since the late 1980's in a number of papers,
including the notable &lt;a href="http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.61.834"&gt;Feng, Kane, Lee, and Stone&lt;/a&gt; paper and &lt;a href="http://www.sciencedirect.com/science/article/pii/037596019090615U#"&gt;Freund's
discussion&lt;/a&gt; of using cross-correlating two speckle patterns, one of
which is a reference wave, to do a sort of holographic imaging. The
recent work continues from these original ideas and applies them to
microscopy.
&lt;/p&gt;

&lt;p&gt;
One article appeared on the arXiv and is from the &lt;a href="http://arxiv.org/abs/1410.2079"&gt;Mosk and Lagendijk&lt;/a&gt;
camp. In this article, they exploit the optical memory effect,
whereby a speckle pattern generated by the multiple scattering of a
plane wave by a random slab is simply translated as the angle of
incidence of the plane wave is varied. If a thin fluorescent target
is placed directly on the opposite face of the scattering slab, it
will be excited by the speckle pattern and emit a fluorescence
signal that can be captured by an objective. Changing the angle of
incidence of the plane wave then allows for multiple points to scan
the sample in parallel. Ultimately, a number of images are taken
with the sample illuminated by several transversally shifted speckle
patterns and the resulting 4D data cube (corresponding to the
sample's x-y dimensions and the two tilt angles of the incident
plane wave) is used to render an image with improved resolution.
&lt;/p&gt;

&lt;p&gt;
As stated in the article's title, the resolution improvement is
relative to that of a widefield microscope. They obtain an effective
point spread function of about 140 nm and a field of view of 10
microns by 10 microns.
&lt;/p&gt;

&lt;p&gt;
So how does the technique work? My first thought was that the memory
effect is simply another way of saying that the speckle pattern acts
as a number of confocal-like point sources, which essentially means
this is something of a parallelized confocal microscope. However,
I'm not sure this is correct for two reasons: 1) there is no
detection pin hole, and 2) the angle of incidence of the plane wave
is scanned over a small angular range so that the speckle pattern is
simply translated. If the angle of incidence is so great that the
linear change in phase of the plane wave is greater than roughly the
size of the scattering slab, the speckle pattern is no longer simply
translated but changes completely.
&lt;/p&gt;

&lt;p&gt;
In reality, the ultimate resolution of this technique is the average
speckle grain size, which can't be less than about half the
wavelength. This suggests that the angular spectrum of the speckle
pattern is what determines the resolution improvement.
&lt;/p&gt;

&lt;p&gt;
The speckle pattern in a region bounded by the maximum extent of the
memory effect has a fixed angular spectrum and translating the
speckle pattern only changes the phase of the spectrum. So, scanning
a target with a speckle pattern produces beat patterns containing
high spatial frequency information that can propgate on the low
spatial frequency waves that reach the objective. Translating the
speckle pattern then performs a sort of phase-shifting
interferometry that allows for both intensity and phase retrieval of
the object.
&lt;/p&gt;

&lt;p&gt;
Importantly, if the speckle pattern is scanned outside the range of
the memory effect, the speckle's angular spectrum within the region
changes completely so that the original reference wave is lost. The
fact that the object is fluorescent and not simply scattering the
light shouldn't matter if the fluorescence intensity is linear with
the excitation light intensity. However, if the fluorescence has
saturated at the average intensity of the speckle pattern, then I'm
not exactly sure that this technique will work (though maybe some
sort of nonlinear structured illumination microscopy could be
achieved).
&lt;/p&gt;

&lt;p&gt;
Overall it's a neat demonstration and worth the exercise to
understand it, though I'm doubtful that at this point it would be
useful for applications in the life sciences. This is because the
resolution isn't that much better than a spinning disk confocal
microscope, which has a larger field of view and would arguably be
easier to use by biologists.
&lt;/p&gt;</description><category>microscopy</category><category>optics</category><guid>http://kmdouglass.github.io/posts/confocal-by-scattering.html</guid><pubDate>Wed, 08 Oct 2014 22:00:00 GMT</pubDate></item><item><title>An accidental half-wave plate</title><link>http://kmdouglass.github.io/posts/half-wave plate.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
Recently in the lab we made a seemingly minor change to one of our
microscopes that led to a minor problem that lasted for about a
week. Briefly, we introduced a mirror following a periscope that
raises the plane that the laser beams travel in parallel to the
table. The periscope is necessary to bringthe beams out of the plane
containing the beam-combining optics and into the TIRF module port
on our Zeiss inverted microscope for epi-illumination.
&lt;/p&gt;

&lt;p&gt;
We had introduced the mirror to give us one more degree of freedom
for steering the beam, but to do so we had to move the periscope and
turn one of the mirrors by 90 degrees. Unfortunately, after doing
this we found that the optical power leaving the microscope
objective had dropped by a factor of 10, which was insufficient to
do STORM imaging.
&lt;/p&gt;

&lt;p&gt;
We eventually determined that the Zeiss TIRF module contained a
polarizing beam splitter that combined the laser beams with white
light from another port at 90 degrees to the beam path. In the
newsetup, we placed a broadband halfwave plate before the TIRF
module, rotated it while watching a power meter, and were finally
able to get even more power leaving the objective than with the old
setup.
&lt;/p&gt;

&lt;p&gt;
So what had happened to cause the decrease in power by adding
another mirror, and why did introducing the halfwave plate fix the
problem? As it turns out, you can rotate the polarization by 90
degrees using only a pair of mirrors to redirect the beam into a
plane parallel to its original plane of travel but into a direction
perpendicular to its original one. This was what we were doing
before we introduced the mirror. After the new mirror was put in
place, we rotated a mirror of the periscope, which effectively
rotated the laser beam's polarization back into its original
orientation. And, since there was a polarizing beam splitter inside
the TIRF module, the new beam polarization was transmitted through
the module instead of being reflected towards the objective.
&lt;/p&gt;

&lt;p&gt;
The picture below describes how this can happen. Note that the final
beam trajectory is in the direction pointing &lt;i&gt;into&lt;/i&gt; the screen.
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="http://kmdouglass.github.io/images/polarization_rotator.png" alt="polarization_rotator.png"&gt;&lt;/p&gt;
&lt;/div&gt;</description><category>optics</category><guid>http://kmdouglass.github.io/posts/half-wave plate.html</guid><pubDate>Tue, 05 Aug 2014 22:00:00 GMT</pubDate></item><item><title>The art of alignment</title><link>http://kmdouglass.github.io/posts/alignment.html</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;
I've been working on realigning one of our STORM microscopy setups
and moving the other over the past couple weeks. As a result, I've
been doing a lot more alignment lately than I normally do, which has
got me thinking about alignment in general.
&lt;/p&gt;

&lt;p&gt;
I've worked in optics labs for nearly ten years now, but I was never
systematically taught how to do alignments. Eventually, I just
figured it out from asking and watching lots of others do it. This
part of experimental optics seems to be something that's passed down
and shared across generations of PhD students and post-docs, like
some sort of cultural heritage preserved by spoken word.
&lt;/p&gt;

&lt;p&gt;
Unfortunately, this fact means that written tutorials or alternative
resources on alignment are scarce. This in turn leads to frustration
for new students who can't easily find someone to show them how to
align a system. I was lucky in my training because both my
undergraduate and graduate programs were in centers dedicated to the
study of optics and optical engineering. Resources were
plentiful. But for a biologist working with a custom microscope
using multiple laser lines, finding someone who is competent in
optics to help realign their system can be highly unlikely.
&lt;/p&gt;

&lt;p&gt;
So how can people who don't have a background in optics be trained
in alignment? I'm certain that written tutorials are &lt;i&gt;not&lt;/i&gt; the best
tool, since aligning an optics setup is a very hands-on job. But, a
tutorial that makes explicit the principles of alignment might make
a good starting point for others.
&lt;/p&gt;

&lt;p&gt;
What are these principles? Below is my rough, initial list that
applies to aligning laser beams. Aligning incoherent systems is
another beast altogether.
&lt;/p&gt;

&lt;ol class="org-ol"&gt;&lt;li&gt;The angle of the beam propagation is just as important as the
position of the beam in a plane perpendicular to the table. In
other words, just because a beam is going through a stopped-down
iris doesn't mean it's traveling where you think it is.
&lt;/li&gt;
&lt;li&gt;You need to measure the beam position in two planes to determine
its angle of propagation.
&lt;/li&gt;
&lt;li&gt;Use "layers of abstraction" to divide sections of the setup into
independent units.
&lt;/li&gt;
&lt;li&gt;Use paired mirrors to get enough degrees of freedom to establish
these abstraction layers.
&lt;/li&gt;
&lt;li&gt;Design feedback mechanisms into the system. In other words, keep
some irises or alignment marks on the walls in place to aid in
future realignments.
&lt;/li&gt;
&lt;/ol&gt;</description><category>optics</category><guid>http://kmdouglass.github.io/posts/alignment.html</guid><pubDate>Sun, 26 Jan 2014 23:00:00 GMT</pubDate></item></channel></rss>